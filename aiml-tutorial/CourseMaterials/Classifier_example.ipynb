{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will use the MNIST hand-written digit dataset to train a classification model. We start again by loading required modules."
      ],
      "metadata": {
        "id": "QySjkfwA1hKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout"
      ],
      "metadata": {
        "id": "e6iQql461uTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the MNIST dataset with the following code and print the shapes."
      ],
      "metadata": {
        "id": "3cKEAQ-Q2D6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, l_train), (x_test, l_test) = mnist.load_data()\n",
        "print('x_train shape: ',x_train.shape)\n",
        "print('l_train shape: ',l_train.shape)\n",
        "print('x_test shape: ',x_test.shape)\n",
        "print('l_test shape: ',l_test.shape)"
      ],
      "metadata": {
        "id": "zWD4KQxQ2EOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see from the previous output that the `x` variables have shapes (28,28) which are the black-white pixels for the small images. There are 60,000 training and 10,000 test samples."
      ],
      "metadata": {
        "id": "cShn-GZ-2Ebn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will be dealing with another fully-connected model (using Dense layers). We must now reshape the (28,28) into (28*28) or (784)."
      ],
      "metadata": {
        "id": "nDMFN6Ug3DTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_new = x_train.reshape(x_train.shape[0],28*28)\n",
        "x_test_new = x_test.reshape(x_test.shape[0],28*28)"
      ],
      "metadata": {
        "id": "B09nmswa2EjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will be doing something new. This is a different type of classification problem where we want to predict probability that an image is a 0, a 1, a 2, etc. So we are going to turn the outputs from integers (0-9) into a vector of length 10. If the number is 0, it becomes [1,0,0,0,0,0,0,0,0,0]. If the number is a 1, it becomes [0,1,0,0,0,0,0,0,0,0]. This continues up to 9. We can do this easily using TensorFlow's `to_categorical` function."
      ],
      "metadata": {
        "id": "B4KXFDX62ErG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_train_new = tf.keras.utils.to_categorical(l_train, num_classes=10, dtype='float32')\n",
        "l_test_new = tf.keras.utils.to_categorical(l_test, num_classes=10, dtype='float32')\n",
        "print(l_train_new.shape)\n",
        "print(l_test_new.shape)"
      ],
      "metadata": {
        "id": "lUOSUASZ2EyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see this in the data itself, we will look at the first training sample. Print the original label, then the categorical label, then plot the image. Note: we use the `binary_r` colormap which is a grayscale colormap with the lowest values being dark and the highest values being light."
      ],
      "metadata": {
        "id": "W2n18TCh2E4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(l_train[0])\n",
        "print(l_train_new[0])\n",
        "plt.imshow(x_train_new[0].reshape(28,28),cmap='binary_r');"
      ],
      "metadata": {
        "id": "bNF1i_8C2E-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are getting ready to develop our ML model. The last step in data preparation is to normalize our input data. Images have values that range from 0 to 255. Large numbers can cause issues with training since the weights are all initialized to be small. By dividing all values by 255, we get values between 0 and 1. Now, the weights will all be in the general ballpark of where they should be."
      ],
      "metadata": {
        "id": "kJJLsKmx2FEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_new = x_train_new / 255.\n",
        "x_test_new = x_test_new / 255."
      ],
      "metadata": {
        "id": "WgrvTYRR2FLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are defining our model as we did with the TensorFlow perceptron. This time, it is slightly more complicated. After the input layer, we have a layer with 256 neurons and a sigmoid activation function. Then we add dropout which helps prevent the model from overfitting to the training data. The rate is the percentage of neurons that are canceled every time the model is run (0.25 = 25%). The second layer has 64 neurons, sigmoid activation, and 10% dropout. The output layer has 10 neurons (for the 10 categories). The softmax activation function is used for classification outputs, because it normalizes the outputs to become probabilities. The sum of all softmax activations will be 1.0 for 100% probability of being one of the outputs."
      ],
      "metadata": {
        "id": "5QJ03uto5WpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(x_train_new.shape[1],)))\n",
        "model.add(Dense(units=256,activation='sigmoid'))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(units=64,activation='sigmoid'))\n",
        "model.add(Dropout(rate=0.10))\n",
        "model.add(Dense(units=l_train_new.shape[1],activation='softmax'))\n",
        "model.compile(loss='mse',optimizer='sgd')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "VfmOHhfC5WyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit the model to the data for 10 epochs with a larger batch size of 128. We also provide validation data to see how the model does on independent data during training."
      ],
      "metadata": {
        "id": "rhO1Ul1O5W5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train_new,l_train_new,epochs=10,batch_size=128,validation_data=(x_test_new,l_test_new),shuffle=True,verbose=1)"
      ],
      "metadata": {
        "id": "XfcTL5lO5W_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the trained model, we will predict for a random validation sample. Since the prediction will be 10 probabilities, we take the `argmax` or the location in the output with the highest probability to be its prediction. Notice that the model does not perform well. You can keep pressing play to see different predictions."
      ],
      "metadata": {
        "id": "ty_42eFK5XEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = np.random.randint(0,x_test_new.shape[0])\n",
        "pred = model.predict(x_test_new[index:index+1])\n",
        "pred_int = np.argmax(pred)\n",
        "print('Truth:',l_test[index],'Prediction:',pred_int,'(confidence=%.2f%%)'%(100*pred[0,pred_int]))\n",
        "plt.imshow(x_test_new[index].reshape(28,28),cmap='binary_r');"
      ],
      "metadata": {
        "id": "Q4Z8hMPA5XJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A big issue with the previous model is that it does not have an appropriate loss function and optimizer. We were using the same one from the previous simple problem. However, our data is now 784-dimensional and there are 60,000 training images. This type of classification works better with a binary crossentropy loss function. Also we use a more advanced loss function called Adam."
      ],
      "metadata": {
        "id": "kV4kGbvm5XQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(x_train_new.shape[1],)))\n",
        "model.add(Dense(units=256,activation='sigmoid'))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(units=64,activation='relu'))\n",
        "model.add(Dropout(rate=0.10))\n",
        "model.add(Dense(units=l_train_new.shape[1],activation='softmax'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='Adam')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QthW8aPr5XVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit the model the same way we did previously."
      ],
      "metadata": {
        "id": "c5AEJz3G5Xbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train_new,l_train_new,epochs=10,batch_size=128,validation_data=(x_test_new,l_test_new),shuffle=True,verbose=1)"
      ],
      "metadata": {
        "id": "O_ImlrcF5XhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run the random predictions again to see how much more accurate and confident the model is."
      ],
      "metadata": {
        "id": "US4Q3tmyEIem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = np.random.randint(0,x_test_new.shape[0])\n",
        "pred = model.predict(x_test_new[index:index+1])\n",
        "pred_int = np.argmax(pred)\n",
        "print('Truth:',l_test[index],'Prediction:',pred_int,'(confidence=%.2f%%)'%(100*pred[0,pred_int]))\n",
        "plt.imshow(x_test_new[index].reshape(28,28),cmap='binary_r');\n"
      ],
      "metadata": {
        "id": "WAk31UxmEIk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fun, we will add a lot of noise to the image. The standard deviation is about 40%. You can see in the plots that many digits are tough for us to even tell but you may be surprised how well the model works here. Even so, it clearly does not perform as well as it did previously."
      ],
      "metadata": {
        "id": "sDD7eID0EkOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = np.random.randint(0,x_test_new.shape[0])\n",
        "input_clean = x_test_new[index:index+1]\n",
        "input_noisy = input_clean + np.random.normal(0,0.4,size=[input_clean.shape[1]])\n",
        "pred = model.predict(input_noisy)\n",
        "pred_int = np.argmax(pred)\n",
        "print('Truth:',l_test[index],'Prediction:',pred_int,'(confidence=%.2f%%)'%(100*pred[0,pred_int]))\n",
        "plt.imshow(input_noisy.reshape(28,28),cmap='binary_r',vmin=0,vmax=1);\n"
      ],
      "metadata": {
        "id": "-Y8V8n4EEkWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}